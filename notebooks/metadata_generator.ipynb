{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d901e664-e35f-4951-8119-779b1e1e7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pirachi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pirachi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "import cv2\n",
    "import nltk\n",
    "import json\n",
    "from pdfminer.high_level import extract_text as extract_pdf_text\n",
    "from docx import Document\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from docx import Document\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "print(\"Setup done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afc8f33-0778-4910-9e1e-2bcf091d2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    return extract_pdf_text(path)\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_text_from_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def extract_text_auto(path):\n",
    "    ext = path.split('.')[-1].lower()\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(path)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(path)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(path)\n",
    "    elif ext in ['jpg', 'jpeg', 'png']:\n",
    "        return extract_text_from_image(path)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c032513a-dff3-41f4-bb03-320224644aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def split_into_chunks(text, max_chars=1000):\n",
    "    paragraphs = text.split('\\n')\n",
    "    chunks, current = [], ''\n",
    "    for para in paragraphs:\n",
    "        if len(current) + len(para) < max_chars:\n",
    "            current += para + '\\n'\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = para + '\\n'\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "def summarize_text_limited(text, max_chars=1000, max_chunks=3):\n",
    "    chunks = split_into_chunks(text, max_chars=max_chars)\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks[:max_chunks]):\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping chunk {i}: {e}\")\n",
    "    return \"\\n\".join(summaries)\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=top_n)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b552dff6-72f5-4915-8d9b-49ac08a3d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(path):\n",
    "    text = extract_text_auto(path)\n",
    "\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return {\"error\": \"Insufficient text extracted.\"}\n",
    "\n",
    "    lang = detect(text)\n",
    "    summary = summarize_text_limited(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": path.split(\"/\")[-1],\n",
    "        \"extracted_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"language\": lang,\n",
    "        \"summary\": summary,\n",
    "        \"keywords\": keywords.tolist(),\n",
    "        \"num_characters\": len(text),\n",
    "        \"num_words\": len(text.split())\n",
    "    }\n",
    "\n",
    "    output_path = path + \".metadata.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(\"✅ Metadata saved to:\", output_path)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3beabf-af8a-49cc-90e9-e4de7b19425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metadata saved to: ../sample_docs/sample.docx.metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': 'sample.docx',\n",
       " 'extracted_on': '2025-06-24 20:26:54',\n",
       " 'language': 'en',\n",
       " 'summary': ' Alphabet Inc. registered pursuant to Section 12(b) of the SECURITIES EXCHANGE ACT of 1934 . The Securities and Exchange Commission filed a form of 10-Q at the time of the quarter ended September 30, 2024 .\\n Indicate by check mark whether the registrant has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 .\\n As of October 22, 2024, there were 5,843 million shares of Alphabet’s Class A stock outstanding . Indicate by check mark whether the registrant is a shell company (as defined in Rule 12b-2 of the Exchange Act).',\n",
       " 'keywords': ['2023',\n",
       "  '2024',\n",
       "  '30',\n",
       "  'billion',\n",
       "  'class',\n",
       "  'ended',\n",
       "  'google',\n",
       "  'months',\n",
       "  'revenues',\n",
       "  'september'],\n",
       " 'num_characters': 124809,\n",
       " 'num_words': 18792}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_metadata(\"../sample_docs/sample.pdf\")\n",
    "generate_metadata(\"../sample_docs/sample.docx\")\n",
    "# generate_metadata(\"../sample_docs/sample.txt\")\n",
    "# generate_metadata(\"../sample_docs/image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13b197-d252-466c-a85e-79c41d5f327e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20352439-f1bf-4d26-99d3-78e80fee796c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e1ef0-478f-479d-961d-cb5cb5add6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79230da-9002-4ebf-97df-8620fb241f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bc0e8-0971-4903-9b81-c297fee6edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59931d54-6a20-42cc-ab8a-bf995d2b271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bbd01-a71e-49e7-9a71-dc5eec639844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eef44a-77c1-4f3d-a402-57f871705cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4469efe-fc6e-4587-859e-82671dd5559a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c33f01e-e2bf-47ab-9e2c-41f0c19382ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912e7d8-5170-4505-b5a3-4d23807c900d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa9b99-3f88-41ce-9584-523e6f36532b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab7925-f788-49ca-a2a3-43ce5b623293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2553d-294d-4961-9ce8-52016441f06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7a5a8-e304-4cd3-a4b7-bb2179208892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94e3b4-adfd-4d0f-a541-27e72e2663cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f23c6-c307-4d2b-9b02-310f68533563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c669-8a83-493a-9ebd-6bf3c953959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extracting Text\n",
    "# pdf_path = \"../sample_docs/sample.pdf\"\n",
    "\n",
    "# def split_into_chunks(text, max_chars=1000):\n",
    "#     paragraphs = text.split('\\n')\n",
    "#     chunks, current_chunk = [], ''\n",
    "#     for para in paragraphs:\n",
    "#         if len(current_chunk) + len(para) < max_chars:\n",
    "#             current_chunk += para + '\\n'\n",
    "#         else:\n",
    "#             chunks.append(current_chunk.strip())\n",
    "#             current_chunk = para + '\\n'\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk.strip())\n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdac2d7-d7fb-4652-9b60-bb7323dfaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_text_limited(text, summarizer, max_chars=1000, max_chunks=3):\n",
    "#     chunks = split_into_chunks(text, max_chars)\n",
    "#     summaries = []\n",
    "\n",
    "#     for i, chunk in enumerate(chunks[:max_chunks]):\n",
    "#         try:\n",
    "#             summary = summarize_text_limited(text, summarizer, max_chars=1000, max_chunks=3)\n",
    "#             summaries.append(summary)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error summarizing chunk {i}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return \"\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80556d0-18e5-4168-b451-56ec084a3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Main API Function\n",
    "# # ----------------------------\n",
    "\n",
    "# def generate_metadata(path):\n",
    "#     print(f\"\\n📄 Processing: {path}\")\n",
    "#     text = extract_text_auto(path)\n",
    "\n",
    "#     if not text or len(text.strip()) < 50:\n",
    "#         return {\"error\": \"Insufficient content extracted.\"}\n",
    "\n",
    "#     # Load model once\n",
    "#     summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "#     lang = detect(text)\n",
    "#     summary = summarize_text(text, summarizer)\n",
    "#     keywords = extract_keywords(text, top_n=10)\n",
    "\n",
    "#     metadata = {\n",
    "#         \"filename\": os.path.basename(path),\n",
    "#         \"extracted_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         \"language\": lang,\n",
    "#         \"summary\": summary,\n",
    "#         \"keywords\": keywords.tolist() if hasattr(keywords, 'tolist') else list(keywords),\n",
    "#         \"num_characters\": len(text),\n",
    "#         \"num_words\": len(text.split())\n",
    "#     }\n",
    "\n",
    "#     # Save as JSON\n",
    "#     out_path = path + \".metadata.json\"\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         json.dump(metadata, f, indent=4)\n",
    "\n",
    "#     print(f\"✅ Metadata saved to: {out_path}\")\n",
    "#     return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d3b83-8dbb-4ab8-9d4d-3dc45cad8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from metadata_generator import generate_metadata\n",
    "\n",
    "# # Works with any supported file\n",
    "# generate_metadata(\"sample_docs/sample.pdf\")\n",
    "# # generate_metadata(\"sample_docs/sample.docx\")\n",
    "# generate_metadata(\"sample_docs/sample.txt\")\n",
    "# # generate_metadata(\"sample_docs/scan.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f468c-9c9f-49da-86ca-259397591924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
