{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76820c-8dd4-4719-ac3d-6cb9e92f1c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pdfminer.six python-docx pytesseract opencv-python transformers nltk langdetect --quiet\n",
    "!pip install rake_nltk\n",
    "!pip install python-docx\n",
    "!pip install pytesseract opencv-python\n",
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3c19f0-1dc9-44bb-99de-b402a2ca8db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pirachi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pirachi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "import cv2\n",
    "import nltk\n",
    "import json\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import Document\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from docx import Document\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "print(\"Setup done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a34379-8dfe-42da-8cd6-6d1f87eb307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Text\n",
    "pdf_path = \"../sample_docs/sample.pdf\"\n",
    "\n",
    "# def extract_text_from_pdf(path):\n",
    "#     try:\n",
    "#         text = extract_text(path)\n",
    "#         return text.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Error extracting text: {e}\"\n",
    "\n",
    "# # Run extraction\n",
    "# pdf_text = extract_text_from_pdf(pdf_path)\n",
    "# print(\"Text extracted. Preview:\\n\")\n",
    "# print(pdf_text[:1000])\n",
    "\n",
    "# def split_into_chunks(text, max_chars=1000):\n",
    "#     \"\"\"\n",
    "#     Splits the full text into semi-intelligent chunks for summarization.\n",
    "#     Breaks on paragraphs when possible.\n",
    "#     \"\"\"\n",
    "#     paragraphs = text.split('\\n')\n",
    "#     chunks, current_chunk = [], ''\n",
    "\n",
    "#     for para in paragraphs:\n",
    "#         if len(current_chunk) + len(para) < max_chars:\n",
    "#             current_chunk += para + '\\n'\n",
    "#         else:\n",
    "#             chunks.append(current_chunk.strip())\n",
    "#             current_chunk = para + '\\n'\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk.strip())\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "def split_into_chunks(text, max_chars=1000):\n",
    "    paragraphs = text.split('\\n')\n",
    "    chunks, current_chunk = [], ''\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) < max_chars:\n",
    "            current_chunk += para + '\\n'\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + '\\n'\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cc4b6-d5a5-4fc6-8173-fb8842cffc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# # Keep first 1000 tokens or so\n",
    "# chunk = pdf_text[:1000]\n",
    "\n",
    "# summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "# print(\"Summary:\\n\", summary)\n",
    "\n",
    "# def summarize_text(text, max_chars_per_chunk=1000):\n",
    "#     chunks = split_into_chunks(text, max_chars=max_chars_per_chunk)\n",
    "#     summaries = []\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         try:\n",
    "#             result = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "#             summaries.append(result)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipping chunk {i} due to error: {e}\")\n",
    "#             continue\n",
    "#     return \"\\n\".join(summaries)\n",
    "\n",
    "def summarize_text_limited(text, summarizer, max_chars=1000, max_chunks=3):\n",
    "    chunks = split_into_chunks(text, max_chars)\n",
    "    summaries = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks[:max_chunks]):\n",
    "        try:\n",
    "            summary = summarize_text_limited(text, summarizer, max_chars=1000, max_chunks=3)\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing chunk {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return \"\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b48bd-bbdc-442c-993e-099753963501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(path):\n",
    "    print(f\"\\nðŸ“„ Processing: {path}\")\n",
    "    text = extract_text_auto(path)\n",
    "\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return {\"error\": \"Insufficient content extracted.\"}\n",
    "\n",
    "    # Load model once\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "    lang = detect(text)\n",
    "    summary = summarize_text_limited(text, summarizer)\n",
    "    keywords = extract_keywords(text, top_n=10)\n",
    "\n",
    "    metadata = {\n",
    "        \"filename\": os.path.basename(path),\n",
    "        \"extracted_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"language\": lang,\n",
    "        \"summary\": summary,\n",
    "        \"keywords\": keywords.tolist() if hasattr(keywords, 'tolist') else list(keywords),\n",
    "        \"num_characters\": len(text),\n",
    "        \"num_words\": len(text.split())\n",
    "    }\n",
    "\n",
    "    # Save as JSON\n",
    "    out_path = path + \".metadata.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… Metadata saved to: {out_path}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f3091-8dd3-4399-81c8-7c1612d0d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Language\n",
    "language = detect(pdf_text)\n",
    "print(f\"Detected Language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8f0b9-5ca1-4178-a2af-06663735a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=top_n)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "keywords = extract_keywords(pdf_text)\n",
    "print(\"ðŸ”‘ Keywords:\")\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0a868-9876-4911-a171-c8a664c6b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata = {\n",
    "    \"filename\": pdf_path.split(\"/\")[-1],\n",
    "    \"extracted_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"language\": language,\n",
    "    \"summary\": summary,\n",
    "    \"keywords\": keywords.tolist() if hasattr(keywords, 'tolist') else list(keywords),\n",
    "    \"num_characters\": len(pdf_text),\n",
    "    \"num_words\": len(pdf_text.split())\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(\"../sample_docs/sample_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"âœ… Metadata saved as 'sample_metadata.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61b4d7-3745-4542-a0c7-1d2a741516b2",
   "metadata": {},
   "source": [
    "# DOC extrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c9b7f-731e-4f30-b1bc-3cd0de31443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daa02a-44cf-4e46-83b6-bd731a0da2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    try:\n",
    "        doc = Document(path)\n",
    "        full_text = [para.text for para in doc.paragraphs]\n",
    "        return \"\\n\".join(full_text)\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting DOCX: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f250b86-d977-409e-97c4-ac7669993c89",
   "metadata": {},
   "source": [
    "# TXT extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882206-f3d9-4e6f-9292-a4e584406f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting TXT: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53443048-254e-4017-b8a8-b74cb1bb41a6",
   "metadata": {},
   "source": [
    "# OCR for scanned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f0c79-17a2-4f37-bcd8-8907f1bd64fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a422645-dbd9-42cb-97fa-62d743a14ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_text_from_image(path):\n",
    "    try:\n",
    "        image = cv2.imread(path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error in OCR: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3ca6e-abeb-4d9b-ab2e-649def2b6ba8",
   "metadata": {},
   "source": [
    "## Detect file type and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dced6cb-25ee-40f1-b81d-343a9c4d9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_auto(path):\n",
    "    ext = path.split('.')[-1].lower()\n",
    "    if ext == 'pdf':\n",
    "        return extract_text_from_pdf(path)\n",
    "    elif ext == 'docx':\n",
    "        return extract_text_from_docx(path)\n",
    "    elif ext == 'txt':\n",
    "        return extract_text_from_txt(path)\n",
    "    elif ext in ['jpg', 'jpeg', 'png']:\n",
    "        return extract_text_from_image(path)\n",
    "    else:\n",
    "        return \"Unsupported file format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e8d81-003e-4747-ad18-2639c31cfafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../sample_docs/sample.txt\"  # or sample.docx, sample.png, etc.\n",
    "text = extract_text_auto(path)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b30558-c792-4d5f-bdcb-a99b23491b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata(path):\n",
    "    # Extract text from any file\n",
    "    text = extract_text_auto(path)\n",
    "\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return {\"error\": \"Insufficient text extracted from document.\"}\n",
    "\n",
    "    # Language detection\n",
    "    lang = detect(text)\n",
    "\n",
    "    # Summarization (limit input length)\n",
    "    chunk = text[:1000]\n",
    "    summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    # Keywords using TF-IDF\n",
    "    kw = extract_keywords(text, top_n=10)\n",
    "\n",
    "    # Final metadata\n",
    "    metadata = {\n",
    "        \"filename\": path.split(\"/\")[-1],\n",
    "        \"extracted_on\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"language\": lang,\n",
    "        \"summary\": summary,\n",
    "        \"keywords\": kw.tolist() if hasattr(kw, 'tolist') else list(kw),\n",
    "        \"num_characters\": len(text),\n",
    "        \"num_words\": len(text.split())\n",
    "    }\n",
    "\n",
    "    # Save metadata to JSON\n",
    "    output_path = path.replace(\".pdf\", \".json\").replace(\".docx\", \".json\").replace(\".txt\", \".json\").replace(\".png\", \".json\").replace(\".jpg\", \".json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… Metadata generated and saved to: {output_path}\")\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2213c6-418c-44d0-8cc4-76c41d0814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_metadata(\"../sample_docs/sample.pdf\")\n",
    "generate_metadata(\"../sample_docs/sample.txt\")\n",
    "generate_metadata(\"../sample_docs/sample.docx\")\n",
    "# generate_metadata(\"../sample_docs/scan.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae5734-6824-42a8-aceb-45d379a8b15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68577875-e50f-4e5d-876c-2436ff9dc25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
